
@article{maska_cell_2023,
	title = {The {Cell} {Tracking} {Challenge}: 10 years of objective benchmarking},
	volume = {20},
	copyright = {2023 The Author(s)},
	issn = {1548-7105},
	shorttitle = {The {Cell} {Tracking} {Challenge}},
	url = {https://www.nature.com/articles/s41592-023-01879-y},
	doi = {10.1038/s41592-023-01879-y},
	abstract = {The Cell Tracking Challenge is an ongoing benchmarking initiative that has become a reference in cell segmentation and tracking algorithm development. Here, we present a significant number of improvements introduced in the challenge since our 2017 report. These include the creation of a new segmentation-only benchmark, the enrichment of the dataset repository with new datasets that increase its diversity and complexity, and the creation of a silver standard reference corpus based on the most competitive results, which will be of particular interest for data-hungry deep learning-based strategies. Furthermore, we present the up-to-date cell segmentation and tracking leaderboards, an in-depth analysis of the relationship between the performance of the state-of-the-art methods and the properties of the datasets and annotations, and two novel, insightful studies about the generalizability and the reusability of top-performing methods. These studies provide critical practical conclusions for both developers and users of traditional and machine learning-based cell segmentation and tracking algorithms.},
	language = {en},
	number = {7},
	urldate = {2023-09-04},
	journal = {Nature Methods},
	author = {Maška, Martin and Ulman, Vladimír and Delgado-Rodriguez, Pablo and Gómez-de-Mariscal, Estibaliz and Nečasová, Tereza and Guerrero Peña, Fidel A. and Ren, Tsang Ing and Meyerowitz, Elliot M. and Scherr, Tim and Löffler, Katharina and Mikut, Ralf and Guo, Tianqi and Wang, Yin and Allebach, Jan P. and Bao, Rina and Al-Shakarji, Noor M. and Rahmon, Gani and Toubal, Imad Eddine and Palaniappan, Kannappan and Lux, Filip and Matula, Petr and Sugawara, Ko and Magnusson, Klas E. G. and Aho, Layton and Cohen, Andrew R. and Arbelle, Assaf and Ben-Haim, Tal and Raviv, Tammy Riklin and Isensee, Fabian and Jäger, Paul F. and Maier-Hein, Klaus H. and Zhu, Yanming and Ederra, Cristina and Urbiola, Ainhoa and Meijering, Erik and Cunha, Alexandre and Muñoz-Barrutia, Arrate and Kozubek, Michal and Ortiz-de-Solórzano, Carlos},
	month = jul,
	year = {2023},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Computational platforms and environments, Image processing},
	pages = {1010--1020},
	file = {Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\YXHJZSIL\\Maška et al. - 2023 - The Cell Tracking Challenge 10 years of objective.pdf:application/pdf},
}

@article{avi-aharon_differentiable_2023,
	title = {Differentiable {Histogram} {Loss} {Functions} for {Intensity}-based {Image}-to-{Image} {Translation}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2023.3278287},
	abstract = {We introduce the HueNet - a novel deep learning framework for a differentiable construction of intensity (1D) and joint (2D) histograms and present its applicability to paired and unpaired image-to-image translation problems. The key idea is an innovative technique for augmenting a generative neural network by histogram layers appended to the image generator. These histogram layers allow us to define two new histogram-based loss functions for constraining the structural appearance of the synthesized output image and its color distribution. Specifically, the color similarity loss is defined by the Earth Mover's Distance between the intensity histograms of the network output and a color reference image. The structural similarity loss is determined by the mutual information between the output and a content reference image based on their joint histogram. Although the HueNet can be applied to a variety of image-to-image translation problems, we chose to demonstrate its strength on the tasks of color transfer, exemplar-based image colorization, and edges {\textbackslash}to photo, where the colors of the output image are predefined. The code is available at https://github.com/mor-avi-aharon-bgu/HueNet.git},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Avi-Aharon, Mor and Arbelle, Assaf and Raviv, Tammy Riklin},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep learning, Earth Movers Distance, Generators, Histogram Layers, Histograms, Image color analysis, Image edge detection, Image-to-Image Translation, Intensity Histogram Loss Functions, Mutual Information Loss, Semantics, Task analysis},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DoronSerebro\\Zotero\\storage\\QLZXGY8P\\10133915.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\V5YQIZLJ\\Avi-Aharon et al. - 2023 - Differentiable Histogram Loss Functions for Intens.pdf:application/pdf},
}

@inproceedings{buchnik_generating_2023,
	title = {Generating {Artistic} {Images} {Via} {Few}-{Shot} {Style} {Transfer}},
	doi = {10.1109/ICASSPW59220.2023.10193400},
	abstract = {Generating images from a predefined style with heterogeneous and limited data is a challenging task for generative models. This work focuses on the conditional generation of artistic images, aiming to learn from a small set of paintings with high variability how to convert real-world photos into impressionistic paintings with the same given style. We design a few-shot style transfer model using a mixture of diverse one-shot style transfer generative models based on the SinGAN model. The proposed few-shot model coineEnSinGAN utilizes an ensemble of different SinGAN realizations to style transfer realistic photos to their closest painting style, by incorporating a novel aggregation mechanism based on the minimum cosine distance in the latent space of the feature vectors. EnSinGAN generates convincing impressionistic landscape images, and was awarded the first place in the Kaggle competition “I’m something of a painter myself” by being the closest in distribution to the test images.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing} {Workshops} ({ICASSPW})},
	author = {Buchnik, Itay and Berebi, Or and Raviv, Tammy Riklin and Shlezinger, Nir},
	month = jun,
	year = {2023},
	keywords = {Acoustics, Conferences, Data models, few-shot learning, Measurement, Signal processing, Speech processing, Style transfer, Task analysis},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DoronSerebro\\Zotero\\storage\\WGASRV2J\\10193400.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\XKUFBQXX\\Buchnik et al. - 2023 - Generating Artistic Images Via Few-Shot Style Tran.pdf:application/pdf},
}

@inproceedings{ben-haim_deep_2022,
	title = {A {Deep} {Ensemble} {Learning} {Approach} to {Lung} {CT} {Segmentation} for {Covid}-19 {Severity} {Assessment}},
	doi = {10.1109/ICIP46576.2022.9897656},
	abstract = {We present a novel deep learning approach to categorical segmentation of lung CTs of COVID-19 patients. Specifically, we partition the scans into healthy lung tissues, non-lung regions, and two different, yet visually similar, pathological lung tissues, namely, ground-glass opacity and consolidation. This is accomplished via a unique, end-to-end hierarchical network architecture and ensemble learning, which contribute to the segmentation and provide a measure for segmentation uncertainty.The proposed framework achieves competitive results and outstanding generalization capabilities for three COVID-19 datasets. Our method is ranked second in a public Kaggle competition for COVID-19 CT images segmentation. Moreover, segmentation uncertainty regions are shown to correspond to the disagreements between the manual annotations of two different radiologists. Finally, preliminary promising correspondence results are shown for our private dataset when comparing the patients’ COVID-19 severity scores (based on clinical measures), and the segmented lung pathologies. Code and data are available at our repository1.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Ben-Haim, Tal and Sofer, Ron Moshe and Ben-Arie, Gal and Shelef, Ilan and Raviv, Tammy Riklin},
	month = oct,
	year = {2022},
	note = {ISSN: 2381-8549},
	keywords = {Categorical Segmentation, Computed tomography, COVID-19, Deep learning, Deep Learning, Image segmentation, Lung CT, Measurement uncertainty, Pathology, Severity Assessment, Uncertainty},
	pages = {151--155},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DoronSerebro\\Zotero\\storage\\SUU56YD2\\9897656.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\H78Z78N2\\Ben-Haim et al. - 2022 - A Deep Ensemble Learning Approach to Lung CT Segme.pdf:application/pdf},
}

@article{arbelle_dual-task_2022,
	title = {Dual-{Task} {ConvLSTM}-{UNet} for {Instance} {Segmentation} of {Weakly} {Annotated} {Microscopy} {Videos}},
	volume = {41},
	issn = {1558-254X},
	doi = {10.1109/TMI.2022.3152927},
	abstract = {Convolutional Neural Networks (CNNs) are considered state of the art segmentation methods for biomedical images in general and microscopy sequences of living cells, in particular. The success of the CNNs is attributed to their ability to capture the structural properties of the data, which enables accommodating complex spatial structures of the cells, low contrast, and unclear boundaries. However, in their standard form CNNs do not exploit the temporal information available in time-lapse sequences, which can be crucial to separating touching and partially overlapping cell instances. In this work, we exploit cell dynamics using a novel CNN architecture which allows multi-scale spatio-temporal feature extraction. Specifically, a novel recurrent neural network (RNN) architecture is proposed based on the integration of a Convolutional Long Short Term Memory (ConvLSTM) network with the U-Net. The proposed ConvLSTM-UNet network is constructed as a dual-task network to enable training with weakly annotated data, in the form of approximate cell centers, termed markers, when the complete cells’ outlines are not available. We further use the fast marching method to facilitate the partitioning of clustered cells into individual connected components. Finally, we suggest an adaptation of the method for 3D microscopy sequences without drastically increasing the computational load. The method was evaluated on the Cell Segmentation Benchmark and was ranked among the top three methods on six submitted datasets. Exploiting the proposed built-in marker estimator we also present state-of-the-art cell detection results for an additional, publicly available, weekly annotated dataset. The source code is available at https://gitlab.com/shaked0/lstmUnet.},
	number = {8},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Arbelle, Assaf and Cohen, Shaked and Raviv, Tammy Riklin},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Annotations, Computer architecture, Deep learning, image segmentation, Image segmentation, image sequences, Microprocessors, microscopy, Microscopy, neural networks, object segmentation, recurrent neural networks, Three-dimensional displays, Training},
	pages = {1948--1960},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DoronSerebro\\Zotero\\storage\\QEW66CHN\\9717246.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\JWEVI7UZ\\Arbelle et al. - 2022 - Dual-Task ConvLSTM-UNet for Instance Segmentation .pdf:application/pdf},
}

@article{ziv_stochastic_2021,
	title = {Stochastic weight pruning and the role of regularization in shaping network structure},
	volume = {462},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221011917},
	doi = {10.1016/j.neucom.2021.08.007},
	abstract = {The pressing need to reduce the capacity of deep neural networks has stimulated the development of network dilution methods and their analysis. In this study we present a framework for neural network pruning by sampling from a probability function that favors the zeroing of smaller parameters. This procedure of stochastically setting network weights to zero is done after each parameter updating step in the network learning algorithm. As part of the proposed framework, we examine the contribution of L1 and L2 regularization to the dynamics of pruning larger network structures such as neurons and filters while optimizing for weight pruning. We then demonstrate the effectiveness of the proposed stochastic pruning framework when used together with regularization terms for different network architectures and image analysis tasks. Specifically, we show that using our method we can successfully remove more than 50\% of the channels/filters in VGG-16 and MobileNetV2 for CIFAR10 classification; in ResNet56 for CIFAR100 classification; in a U-Net for instance segmentation of biological cells; and in a CNN model tailored for COVID-19 detection. For these filter-pruned networks, we also present competitive weight pruning results while maintaining the accuracy levels of the original, dense networks.},
	urldate = {2023-09-04},
	journal = {Neurocomputing},
	author = {Ziv, Yael and Goldberger, Jacob and Riklin Raviv, Tammy},
	month = oct,
	year = {2021},
	keywords = {COVID-19, Neural network compression, Node pruning, Pruning dynamics, Weight decay, Weight pruning},
	pages = {555--567},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\TSLD43AX\\Ziv et al. - 2021 - Stochastic weight pruning and the role of regulari.pdf:application/pdf},
}

@article{gordon_atlas_2021,
	title = {An atlas of classifiers—a machine learning paradigm for brain {MRI} segmentation},
	volume = {59},
	issn = {1741-0444},
	url = {https://doi.org/10.1007/s11517-021-02414-x},
	doi = {10.1007/s11517-021-02414-x},
	abstract = {We present the Atlas of Classifiers (AoC)—a conceptually novel framework for brain MRI segmentation. The AoC is a spatial map of voxel-wise multinomial logistic regression (LR) functions learned from the labeled data. Upon convergence, the resulting fixed LR weights, a few for each voxel, represent the training dataset. It can, therefore, be considered as a light-weight learning machine, which despite its low capacity does not underfit the problem. The AoC construction is independent of the actual intensities of the test images, providing the flexibility to train it on the available labeled data and use it for the segmentation of images from different datasets and modalities. In this sense, it does not overfit the training data, as well. The proposed method has been applied to numerous publicly available datasets for the segmentation of brain MRI tissues and is shown to be robust to noise and outreach commonly used methods. Promising results were also obtained for multi-modal, cross-modality MRI segmentation. Finally, we show how AoC trained on brain MRIs of healthy subjects can be exploited for lesion segmentation of multiple sclerosis patients.},
	language = {en},
	number = {9},
	urldate = {2023-09-04},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Gordon, Shiri and Kodner, Boris and Goldfryd, Tal and Sidorov, Michael and Goldberger, Jacob and Raviv, Tammy Riklin},
	month = sep,
	year = {2021},
	keywords = {Brain MRI, Logistic regression classifiers, Machine learning, MS-lesions, Segmentation},
	pages = {1833--1849},
	file = {Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\KFCIZL3D\\Gordon et al. - 2021 - An atlas of classifiers—a machine learning paradig.pdf:application/pdf},
}

@inproceedings{goldfryd_deep_2021,
	title = {Deep {Semi}-{Supervised} {Bias} {Field} {Correction} {Of} {Mr} {Images}},
	doi = {10.1109/ISBI48211.2021.9433889},
	abstract = {A bias field is an artifact inherent to MRI scanners which is manifested by a smooth intensity variation across the scans. We present an innovative generative approach to address the inverse problem of bias field estimation and removal in a semi-supervised manner. The key contribution is the construction of a compound framework of four interacting, adversarial neural networks. Specifically, we simultaneously train a pair of neural networks, one for the reconstruction of the plain bias field and the other for the reconstruction of a bias-free MRI scan, such that the output of each together with the input biased scans define the loss of the other network. A third network, trained as a bias-field discriminator provides an additional loss to the bias field generator while an MRI segmentation network provides an additional loss to the bias-free MRI generator. We trained and validated our framework using real MRI scans with simulated bias fields and tested it on publicly available brain data-sets as well as private data yielding results competitive with state-of-the-art methods. Code is available upon request.},
	booktitle = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Goldfryd, Tal and Gordon, Shiri and Raviv, Tammy Riklin},
	month = apr,
	year = {2021},
	note = {ISSN: 1945-8452},
	keywords = {Adversarial Neural Networks, Bias field correction, Biological neural networks, Brain MRI, Compounds, Deep Learning, Estimation, Generators, Image reconstruction, Inverse problems, Magnetic resonance imaging},
	pages = {1836--1840},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DoronSerebro\\Zotero\\storage\\VQ622YJB\\9433889.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\78BLLMQJ\\Goldfryd et al. - 2021 - Deep Semi-Supervised Bias Field Correction Of Mr I.pdf:application/pdf},
}

@misc{ben-haim_graph_2022,
	title = {Graph {Neural} {Network} for {Cell} {Tracking} in {Microscopy} {Videos}},
	url = {http://arxiv.org/abs/2202.04731},
	abstract = {We present a novel graph neural network (GNN) approach for cell tracking in high-throughput microscopy videos. By modeling the entire time-lapse sequence as a direct graph where cell instances are represented by its nodes and their associations by its edges, we extract the entire set of cell trajectories by looking for the maximal paths in the graph. This is accomplished by several key contributions incorporated into an end-to-end deep learning framework. We exploit a deep metric learning algorithm to extract cell feature vectors that distinguish between instances of different biological cells and assemble same cell instances. We introduce a new GNN block type which enables a mutual update of node and edge feature vectors, thus facilitating the underlying message passing process. The message passing concept, whose extent is determined by the number of GNN blocks, is of fundamental importance as it enables the `flow' of information between nodes and edges much behind their neighbors in consecutive frames. Finally, we solve an edge classification problem and use the identified active edges to construct the cells' tracks and lineage trees. We demonstrate the strengths of the proposed cell tracking approach by applying it to 2D and 3D datasets of different cell types, imaging setups, and experimental conditions. We show that our framework outperforms current state-of-the-art methods on most of the evaluated datasets. The code is available at our repository: https://github.com/talbenha/cell-tracker-gnn.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Ben-Haim, Tal and Raviv, Tammy Riklin},
	month = jul,
	year = {2022},
	note = {arXiv:2202.04731 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\DoronSerebro\\Zotero\\storage\\PVGEPT2W\\2202.html:text/html;Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\3UPD2MHZ\\Ben-Haim and Raviv - 2022 - Graph Neural Network for Cell Tracking in Microsco.pdf:application/pdf},
}

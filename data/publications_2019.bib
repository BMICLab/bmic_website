
@misc{avi-aharon_hue-net_2019,
	title = {Hue-{Net}: {Intensity}-based {Image}-to-{Image} {Translation} with {Differentiable} {Histogram} {Loss} {Functions}},
	shorttitle = {Hue-{Net}},
	url = {http://arxiv.org/abs/1912.06044},
	doi = {10.48550/arXiv.1912.06044},
	abstract = {We present the Hue-Net - a novel Deep Learning framework for Intensity-based Image-to-Image Translation. The key idea is a new technique termed network augmentation which allows a differentiable construction of intensity histograms from images. We further introduce differentiable representations of (1D) cyclic and joint (2D) histograms and use them for defining loss functions based on cyclic Earth Mover's Distance (EMD) and Mutual Information (MI). While the Hue-Net can be applied to several image-to-image translation tasks, we choose to demonstrate its strength on color transfer problems, where the aim is to paint a source image with the colors of a different target image. Note that the desired output image does not exist and therefore cannot be used for supervised pixel-to-pixel learning. This is accomplished by using the HSV color-space and defining an intensity-based loss that is built on the EMD between the cyclic hue histograms of the output and the target images. To enforce color-free similarity between the source and the output images, we define a semantic-based loss by a differentiable approximation of the MI of these images. The incorporation of histogram loss functions in addition to an adversarial loss enables the construction of semantically meaningful and realistic images. Promising results are presented for different datasets.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Avi-Aharon, Mor and Arbelle, Assaf and Raviv, Tammy Riklin},
	month = dec,
	year = {2019},
	note = {arXiv:1912.06044 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\F4XX3KFK\\Avi-Aharon et al. - 2019 - Hue-Net Intensity-based Image-to-Image Translatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DoronSerebro\\Zotero\\storage\\UG8JIQRN\\1912.html:text/html},
}

@misc{levakov_deep_2019,
	title = {From a deep learning model back to the brain - inferring morphological markers and their relation to aging},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/803742v2},
	doi = {10.1101/803742},
	abstract = {We present a Deep Learning framework for the prediction of chronological age from structural MRI scans. Previous findings associate an overestimation of brain age with neurodegenerative diseases and higher mortality rates. However, the importance of brain age prediction goes beyond serving as biomarkers for neurological disorders. Specifically, utilizing convolutional neural network (CNN) analysis to identify brain regions contributing to the prediction can shed light on the complex multivariate process of brain aging. Previous work examined methods to attribute pixel/voxel-wise contributions to the prediction in a single image, resulting in ‘explanation maps’ that were found noisy and unreliable. To address this problem, we developed an inference framework for combining these maps across subjects, thus creating a population-based rather than a subject-specific map. We applied this method to a CNN ensemble trained on predicting subjects’ age from raw T1 brain images of 10,176 subjects. Evaluating the model on an untouched test set resulted in mean absolute error of 3.07 years and a correlation between chronological and predicted age of r=0.98. Using the inference method, we revealed that cavities containing CSF, previously found as general atrophy markers, had the highest contribution for age prediction. Comparing maps derived from different models within the ensemble allowed to assess differences and similarities in brain regions utilized by the model. We showed that this method substantially increased the replicability of explanation maps, converged with results from voxel-based morphometry age studies and highlighted brain regions whose volumetric variability contributed the most to the prediction.
HighlightsCNNs ensemble is shown to estimate “brain age” from sMRI with an MAE of ∼3.1 yearsA novel framework enables to highlight brain regions contributing to the predictionThis framework results in explanation maps showing consistency with the literatureAs sample size increases, these maps show higher inter-sample replicabilityCSF cavities reflecting general atrophy were found as a prominent aging biomarker},
	language = {en},
	urldate = {2023-09-04},
	publisher = {bioRxiv},
	author = {Levakov, Gidon and Rosenthal, Gideon and Shelef, Ilan and Raviv, Tammy Riklin and Avidan, Galia},
	month = nov,
	year = {2019},
	note = {Pages: 803742
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\6PNK2LNH\\Levakov et al. - 2019 - From a deep learning model back to the brain - inf.pdf:application/pdf},
}

@article{benou_combining_2019,
	title = {Combining white matter diffusion and geometry for tract-specific alignment and variability analysis},
	volume = {200},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811919303866},
	doi = {10.1016/j.neuroimage.2019.05.003},
	abstract = {We present a framework for along-tract analysis of white matter (WM) fiber bundles based on diffusion tensor imaging (DTI) and tractography. We introduce the novel concept of fiber-flux density for modeling fiber tracts’ geometry, and combine it with diffusion-based measures to define vector descriptors called Fiber-Flux Diffusion Density (FFDD). The proposed model captures informative features of WM tracts at both the microscopic (diffusion-related) and macroscopic (geometry-related) scales, thus enabling improved sensitivity to subtle structural abnormalities that are not reflected by either diffusion or geometrical properties alone. A key step in this framework is the construction of an FFDD dissimilarity measure for sub-voxel alignment of fiber bundles, based on the fast marching method (FMM). The obtained aligned WM tracts enable meaningful inter-subject comparisons and group-wise statistical analysis. Moreover, we show that the FMM alignment can be generalized in a straight forward manner to a single-shot co-alignment of multiple fiber bundles. The proposed alignment technique is shown to outperform a well-established, commonly used DTI registration algorithm. We demonstrate the FFDD framework on the Human Connectome Project (HCP) diffusion MRI dataset, as well as on two different datasets of contact sports players. We test our method using longitudinal scans of a basketball player diagnosed with a traumatic brain injury, showing compatibility with structural MRI findings. We further perform a group study comparing mid- and post-season scans of 13 active football players exposed to repetitive head trauma, to 17 non-player control (NPC) subjects. Results reveal statistically significant FFDD differences (p-values{\textless}0.05) between the groups, as well as increased abnormalities over time at spatially-consistent locations within several major fiber tracts of football players.},
	urldate = {2023-09-04},
	journal = {NeuroImage},
	author = {Benou, Itay and Veksler, Ronel and Friedman, Alon and Raviv, Tammy Riklin},
	month = oct,
	year = {2019},
	keywords = {Diffusion MRI, Fiber bundle, Mild traumatic brain injury, Tract-specific analysis, Tractography registration, White matter},
	pages = {674--689},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\VUGMAPX6\\Benou et al. - 2019 - Combining white matter diffusion and geometry for .pdf:application/pdf},
}

@inproceedings{benou_deeptract_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepTract}: {A} {Probabilistic} {Deep} {Learning} {Framework} for {White} {Matter} {Fiber} {Tractography}},
	isbn = {978-3-030-32248-9},
	shorttitle = {{DeepTract}},
	doi = {10.1007/978-3-030-32248-9_70},
	abstract = {We present DeepTract, a deep-learning framework for estimating white matter fibers orientation and streamline tractography. We adopt a data-driven approach for fiber reconstruction from diffusion-weighted images (DWI), which does not assume a specific diffusion model. We use a recurrent neural network for mapping sequences of DWI values into probabilistic fiber orientation distributions. Based on these estimations, our model facilitates both deterministic and probabilistic streamline tractography. We quantitatively evaluate our method using the Tractometer tool, demonstrating competitive performance with state-of-the-art classical and machine learning based tractography algorithms. We further present qualitative results of bundle-specific probabilistic tractography obtained using our method. The code is publicly available at: https://github.com/itaybenou/DeepTract.git.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Benou, Itay and Riklin Raviv, Tammy},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	pages = {626--635},
	file = {Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\FUNK6DAH\\Benou and Riklin Raviv - 2019 - DeepTract A Probabilistic Deep Learning Framework.pdf:application/pdf},
}

@article{gilad_fully_2018,
	title = {Fully unsupervised symmetry-based mitosis detection in time-lapse cell microscopy},
	volume = {35},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/bty1034},
	doi = {10.1093/bioinformatics/bty1034},
	abstract = {Cell microscopy datasets have great diversity due to variability in cell types, imaging techniques and protocols. Existing methods are either tailored to specific datasets or are based on supervised learning, which requires comprehensive manual annotations. Using the latter approach, however, poses a significant difficulty due to the imbalance between the number of mitotic cells with respect to the entire cell population in a time-lapse microscopy sequence.We present a fully unsupervised framework for both mitosis detection and mother–daughters association in fluorescence microscopy data. The proposed method accommodates the difficulty of the different cell appearances and dynamics. Addressing symmetric cell divisions, a key concept is utilizing daughters’ similarity. Association is accomplished by defining cell neighborhood via a stochastic version of the Delaunay triangulation and optimization by dynamic programing. Our framework presents promising detection results for a variety of fluorescence microscopy datasets of different sources, including 2D and 3D sequences from the Cell Tracking Challenge.Code is available in github (github.com/topazgl/mitodix).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2023-09-04},
	journal = {Bioinformatics},
	author = {Gilad, Topaz and Reyes, Jose and Chen, Jia-Yun and Lahav, Galit and Riklin Raviv, Tammy},
	month = aug,
	year = {2018},
	pages = {2644--2653},
	file = {Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\YFJZRXI4\\Gilad et al. - 2018 - Fully unsupervised symmetry-based mitosis detectio.pdf:application/pdf},
}

@misc{shwartzman_worrisome_2020,
	title = {The {Worrisome} {Impact} of an {Inter}-rater {Bias} on {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/1906.11872},
	doi = {10.48550/arXiv.1906.11872},
	abstract = {The problem of inter-rater variability is often discussed in the context of manual labeling of medical images. The emergence of data-driven approaches such as Deep Neural Networks (DNNs) brought this issue of raters' disagreement to the front-stage. In this paper, we highlight the issue of inter-rater bias as opposed to random inter-observer variability and demonstrate its influence on DNN training, leading to different segmentation results for the same input images. In fact, lower overlap scores are obtained between the outputs of a DNN trained on annotations of one rater and tested on another. Moreover, we demonstrate that inter-rater bias in the training examples is amplified and becomes more consistent, considering the segmentation predictions of the DNNs' test data. We support our findings by showing that a classifier-DNN trained to distinguish between raters based on their manual annotations performs better when the automatic segmentation predictions rather than the actual raters' annotations were tested. For this study, we used two different datasets: the ISBI 2015 Multiple Sclerosis (MS) challenge dataset, including MRI scans each with annotations provided by two raters with different levels of expertise; and Intracerebral Hemorrhage (ICH) CT scans with manual and semi-manual segmentations. The results obtained allow us to underline a worrisome clinical implication of a DNN bias induced by an inter-rater bias during training. Specifically, we present a consistent underestimate of MS-lesion loads when calculated from segmentation predictions of a DNN trained on input provided by the less experienced rater. In the same manner, the differences in ICH volumes calculated based on outputs of identical DNNs, each trained on annotations from a different source are more consistent and larger than the differences in volumes between the manual and semi-manual annotations used for training.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Shwartzman, Or and Gazit, Harel and Shelef, Ilan and Riklin-Raviv, Tammy},
	month = may,
	year = {2020},
	note = {arXiv:1906.11872 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\ZKRDJUEV\\Shwartzman et al. - 2020 - The Worrisome Impact of an Inter-rater Bias on Neu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DoronSerebro\\Zotero\\storage\\2A2TVIGR\\1906.html:text/html},
}

@misc{arbelle_qanet_2019,
	title = {{QANet} -- {Quality} {Assurance} {Network} for {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1904.08503},
	doi = {10.48550/arXiv.1904.08503},
	abstract = {We introduce a novel Deep Learning framework, which quantitatively estimates image segmentation quality without the need for human inspection or labeling. We refer to this method as a Quality Assurance Network -- QANet. Specifically, given an image and a `proposed' corresponding segmentation, obtained by any method including manual annotation, the QANet solves a regression problem in order to estimate a predefined quality measure with respect to the unknown ground truth. The QANet is by no means yet another segmentation method. Instead, it performs a multi-level, multi-feature comparison of an image-segmentation pair based on a unique network architecture, called the RibCage. To demonstrate the strength of the QANet, we addressed the evaluation of instance segmentation using two different datasets from different domains, namely, high throughput live cell microscopy images from the Cell Segmentation Benchmark and natural images of plants from the Leaf Segmentation Challenge. While synthesized segmentations were used to train the QANet, it was tested on segmentations obtained by publicly available methods that participated in the different challenges. We show that the QANet accurately estimates the scores of the evaluated segmentations with respect to the hidden ground truth, as published by the challenges' organizers. The code is available at: TBD.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Arbelle, Assaf and Elul, Eliav and Raviv, Tammy Riklin},
	month = nov,
	year = {2019},
	note = {arXiv:1904.08503 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\WMNI5ABT\\Arbelle et al. - 2019 - QANet -- Quality Assurance Network for Image Segme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DoronSerebro\\Zotero\\storage\\5LEVRNRF\\1904.html:text/html},
}

@inproceedings{arbelle_microscopy_2019,
	title = {Microscopy {Cell} {Segmentation} {Via} {Convolutional} {LSTM} {Networks}},
	doi = {10.1109/ISBI.2019.8759447},
	abstract = {Live cell microscopy sequences exhibit complex spatial structures and complicated temporal behaviour, making their analysis a challenging task. Considering cell segmentation problem, which plays a significant role in the analysis, the spatial properties of the data can be captured using Convolutional Neural Networks (CNNs). Recent approaches show promising segmentation results using convolutional encoder-decoders such as the U-Net. Nevertheless, these methods are limited by their inability to incorporate temporal information, that can facilitate segmentation of individual touching cells or of cells that are partially visible. In order to exploit cell dynamics we propose a novel segmentation architecture which integrates Convolutional Long Short Term Memory (C-LSTM) with the U-Net. The network's unique architecture allows it to capture multi-scale, compact, spatio-temporal encoding in the C-LSTMs memory units. The method was evaluated on the Cell Tracking Challenge and achieved state-of-the-art results (1st on Fluo-N2DH-SIM + and 2nd on DIC-C2DLHeLa datasets) The code is freely available at: https://github.com/arbellea/LSTM-UNet.git.},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	author = {Arbelle, Assaf and Raviv, Tammy Riklin},
	month = apr,
	year = {2019},
	note = {ISSN: 1945-8452},
	keywords = {Computer architecture, Decoding, Image segmentation, Image sequences, Microprocessors, Microscopy, Training},
	pages = {1008--1012},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DoronSerebro\\Zotero\\storage\\GZNF2DCT\\8759447.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DoronSerebro\\Zotero\\storage\\GPFEU2JV\\Arbelle and Raviv - 2019 - Microscopy Cell Segmentation Via Convolutional LST.pdf:application/pdf},
}
